{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "843f6466-17e2-4e81-9772-eca6d6883b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eda8fe13-b260-4482-984f-d6ac77ebcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a602503-3fa8-453f-86e0-776bf02d9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a74cd74-7497-4044-a23c-a7810b7792a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f1cd2db-de49-4b93-9f7d-67b35c278d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ec49e47-1fa0-416a-95b1-476d7a5fedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d81a0184-ea8a-4ddf-b8f9-10abfda17446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d64dd8b3-e902-4923-9054-81d5c204ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dea380f-6878-4968-b084-d279c54b2090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d611d8-0a66-4eb0-bfab-75d64570c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56d8561-af40-4354-870f-c92ee02efd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "# model_name_or_path = 'google/vit-base-patch16-224'\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dff6f9d-59c2-432c-acdb-78f6fe9a991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71e8fcd-9812-466b-b553-f9a51b5fb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = (\"4765063/test\")\n",
    "train_path = (\"4765063/train\")\n",
    "validation_path = (\"4765063/validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40719a09-5016-4824-8f54-9b8a33ea00d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6cc602-c1f4-436b-9989-df826eeca8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    image_path = []\n",
    "    struck =[]\n",
    "    for i in os.listdir(path):\n",
    "        for j in os.listdir(path+\"/\"+i):\n",
    "            if j == \".ipynb_checkpoints\":\n",
    "              continue\n",
    "            image_path.append(path+\"/\"+i+\"/\"+j)\n",
    "            if i == \"struck\":\n",
    "                struck.append(1)\n",
    "            else:\n",
    "                struck.append(0)\n",
    "    return image_path , struck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47632455-b19f-4df6-be69-7e1dd18fea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path,test_struck = get_images(test_path)\n",
    "train_image_path,train_struck = get_images(train_path)\n",
    "validation_image_path,validation_struck = get_images(validation_path)\n",
    "\n",
    "\n",
    "train_image_path = test_image_path+train_image_path\n",
    "train_struck = test_struck+train_struck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8789067a-36f1-47bd-aa78-adc93fa5b462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_struck[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ba496e-f5b1-4e13-81e8-bbbaa49e607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(image_path):\n",
    "    images=[]\n",
    "    for i in image_path:\n",
    "        # print(i)\n",
    "        img = cv2.imread(i)\n",
    "        # converting into grayscale\n",
    "        gray_image = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        # convrting into binaryimage\n",
    "        _, binary_image = cv2.threshold(gray_image, 200, 255, cv2.THRESH_BINARY)\n",
    "        binary_image = cv2.resize(binary_image, (224, 224))\n",
    "        # binary_image = np.expand_dims(binary_image, axis=-1)\n",
    "        binary_image = cv2.merge([binary_image, binary_image, binary_image])\n",
    "        binary_image = binary_image/255\n",
    "        binary_image = torch.from_numpy(binary_image)\n",
    "        images.append(binary_image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eabd7c8-b3e5-47a0-b9f8-9e2e174d344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = image_preprocessing(train_image_path)\n",
    "train_images = torch.stack(train_images)\n",
    "train_images = train_images.permute(0, 3, 1, 2)\n",
    "# train_images = image_preprocessing(train_image_path[:10])\n",
    "validation_images = image_preprocessing(validation_image_path)\n",
    "validation_images = torch.stack(validation_images)\n",
    "validation_images = validation_images.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef575ce5-62e7-4196-aa65-937242d2427b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1134, 1134, 252, 252)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images), len(train_struck),len(validation_images),len(validation_struck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77d660a-8940-4419-8507-cd11d3642e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.DataFrame({'x': train_images, 'y':train_struck })\n",
    "# validation = pd.DataFrame({'x': validation_images, 'y':validation_struck })\n",
    "\n",
    "train = {\n",
    "    'image' : train_images,\n",
    "    'label' :train_struck\n",
    "}\n",
    "\n",
    "validation = {\n",
    "    'image' : validation_images,\n",
    "    'label' : validation_struck\n",
    "}\n",
    "\n",
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "validation_dataset = Dataset.from_dict(validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c71e817-aedd-4df0-8ad9-881fa46410cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "#     labels = torch.tensor([item['labels'] for item in batch])\n",
    "#     return {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([torch.from_numpy(np.array(item['image'])) for item in batch])\n",
    "    # pixel_values = torch.stack([torch.from_numpy(item) for item in batch['x']])\n",
    "    labels = torch.tensor([item['label'] for item in batch])\n",
    "\n",
    "    return {'pixel_values': pixel_values, 'labels': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b184c32f-3b72-49aa-bb96-7d403e2bd3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Vardhan\\AppData\\Local\\Temp\\ipykernel_3528\\3558881001.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"precision\")\n",
      "C:\\Users\\Aditya Vardhan\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from C:\\Users\\Aditya Vardhan\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\precision\\26faf6607f5f6fa666ded33d9e7aa1e8818a9cc6f423514adad4623641d8751c (last modified on Wed Jun  5 23:18:16 2024) since it couldn't be found locally at precision, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"precision\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    references = p.label_ids\n",
    "    return metric.compute(predictions=predictions, references=references)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4142fc79-aca3-4712-ae0e-518b20df7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# # labels = ds['train'].features['labels'].names\n",
    "\n",
    "# model = ViTForImageClassification.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     num_labels=2,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bc7cfb-cfd7-4569-a81e-8f4817a15a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"vitpre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0606f0a-93cb-4e30-9dfe-562a762c6de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path from where to load the model\n",
    "model_directory = \"vitpre\"\n",
    "\n",
    "# Load the model from the saved directory\n",
    "model = ViTForImageClassification.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bf73d33-68d2-4323-8d11-c9515269a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya Vardhan\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./vit-base-beans\",\n",
    "  per_device_train_batch_size=16,\n",
    "  per_device_eval_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=20,\n",
    "  fp16=True,\n",
    "  save_steps=10,\n",
    "  eval_steps=10,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=3,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  # report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d054fed-e8f1-47e0-b46b-25e603bf1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,  # Number of evaluation steps to wait before stopping\n",
    "    early_stopping_threshold=0.01,  # Minimum change to qualify as an improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23ea0753-a78d-4c1b-99f9-fff87ff5bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "def custom_processor(images):\n",
    "    return processor(images, do_normalize = False, return_tensors='pt')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # train_dataset=prepared_ds[\"train\"]                 ,\n",
    "    train_dataset = train_dataset,\n",
    "    # eval_dataset=prepared_ds[\"validation\"],\n",
    "    eval_dataset = validation_dataset,\n",
    "    # tokenizer = custom_processor\n",
    "    callbacks = [early_stopping_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8968aaf-8eef-4cff-8446-68c05b4bd5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 160/1420 1:31:54 < 12:12:55, 0.03 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.146019</td>\n",
       "      <td>0.976744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.218500</td>\n",
       "      <td>0.188655</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.131600</td>\n",
       "      <td>0.061196</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.265167</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.141773</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.206900</td>\n",
       "      <td>0.037538</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.954198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.113450</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>0.123448</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.072103</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.017658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.143631</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.011280</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.051202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.018780</td>\n",
       "      <td>0.992126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =      2.2535\n",
      "  total_flos               = 184466620GF\n",
      "  train_loss               =      0.1075\n",
      "  train_runtime            =  1:32:38.76\n",
      "  train_samples_per_second =        4.08\n",
      "  train_steps_per_second   =       0.255\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23bf14cc-ec22-4224-9533-911f3a87a634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 01:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     2.2535\n",
      "  eval_loss               =     0.0113\n",
      "  eval_precision          =        1.0\n",
      "  eval_runtime            = 0:02:00.21\n",
      "  eval_samples_per_second =      2.096\n",
      "  eval_steps_per_second   =      0.133\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(validation_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b73889-feef-47ee-a13b-727d711b29c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
